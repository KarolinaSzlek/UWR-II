{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAiECkYEaZn1"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "## Important notes\n",
    "**Submission deadline:**\n",
    "* **Thursday, 12.03.2020**\n",
    "\n",
    "**Points: 13 + 2bp**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bb0zN1GvapSt"
   },
   "source": [
    "This assignment is meant to test your skills in course pre-requisites:  Scientific Python programming and  Machine Learning. If it is hard, I strongly advise you to drop the course.\n",
    "\n",
    "Please use GitHub’s [pull requests](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests) and issues to send corrections!\n",
    "\n",
    "You can solve the assignment in any system you like, but we encourage you to try out [Google Colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import re\n",
    "import scipy.optimize as sopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIJIJDwgZrOI"
   },
   "source": [
    "1. **[1p]** Download data competition from a Kaggle competition on sentiment prediction from [[https://www.kaggle.com/c/CountVectorizerent-analysis-on-movie-reviews/data](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)].  Keep only full sentences, i.e. for each `SenteceId` keep only the entry with the lowest `PhraseId`.  Use first 7000 sentences as a `train set` and the remaining 1529 sentences as the `test set`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(s):\n",
    "    return re.sub('\\ *\\W\\ *', ' ', s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "df = df.groupby(['SentenceId'], as_index=False).agg({'PhraseId' : 'min',\n",
    "                                               'Phrase' : 'first',\n",
    "                                               'Sentiment' : 'first'})\n",
    "df['Sentiment'] /= 4\n",
    "df = df.drop(['PhraseId', 'SentenceId'], axis=1)\n",
    "df['Phrase'] = df['Phrase'].apply(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:7000]\n",
    "test_df = df.iloc[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this quiet introspective and entertaining inde...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>even fans of ismail merchant s work i suspect ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a positively thrilling combination of ethnogra...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aggressive self glorification and a manipulati...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment\n",
       "0  a series of escapades demonstrating the adage ...       0.25\n",
       "1  this quiet introspective and entertaining inde...       1.00\n",
       "2  even fans of ismail merchant s work i suspect ...       0.25\n",
       "3  a positively thrilling combination of ethnogra...       0.75\n",
       "4  aggressive self glorification and a manipulati...       0.25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **[1p]** Prepare the data for logistic regression:\n",
    "\tMap the sentiment scores $0,1,2,3,4$ to a probability of the sentence being by setting $p(\\textrm{positive}) = \\textrm{sentiment}/4$.\n",
    "\tBuild a dictionary of most frequent 20000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [00:01<00:00, 5522.38it/s]\n"
     ]
    }
   ],
   "source": [
    "word_count = defaultdict(int)\n",
    "for i in trange(len(train_df), position=0, leave=True):\n",
    "    for w in train_df.iloc[i]['Phrase'].split():\n",
    "        word_count[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = dict(sorted(word_count.items(), key=lambda kv: kv[1], reverse=True)[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **[3p]** Treat each document as a bag of words. e.g. if the vocabulary is \n",
    "\t```\n",
    "\t0: the\n",
    "\t1: good\n",
    "\t2: movie\n",
    "\t3: is\n",
    "\t4: not\n",
    "\t5: a\n",
    "\t6: funny\n",
    "\t```\n",
    "\tThen the encodings can be:\n",
    "\t```\n",
    "\tgood:                           [0,1,0,0,0,0,0]\n",
    "\tnot good:                       [0,1,0,0,1,0,0] \n",
    "\tthe movie is not a funny movie: [1,0,2,1,1,1,1]\n",
    "\t```\n",
    "    Train a logistic regression model to predict the sentiment. Compute the correlation between the predicted probabilities and the sentiment. Record the most positive and negative words.\n",
    "    Please note that in this model each word gets its sentiment parameter $S_w$ and the score for a sentence is \n",
    "    $$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}S_w$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCountVectorizer():\n",
    "    \n",
    "    def __init__(self, idx_vocabulary=None, is_negation=False, is_enhance=False, \n",
    "                 neg_words=[], enh_words=[], enh_mult=2):\n",
    "        self.is_negation = is_negation\n",
    "        self.is_enhance = is_enhance\n",
    "        self.idx_vocabulary = idx_vocabulary\n",
    "        self.neg_words = neg_words\n",
    "        self.enh_words = enh_words\n",
    "        self.enh_mult = enh_mult\n",
    "        \n",
    "    def __find_idx_vocabulary(self, sentences):\n",
    "        voc = set()\n",
    "        for s in sentences:\n",
    "            for w in s.split():\n",
    "                if w.isalpha():\n",
    "                    voc.add(w)\n",
    "        self.idx_vocabulary = {w:i for i, w in enumerate(list(voc))}\n",
    "    \n",
    "    def transform(self, sentences):\n",
    "        if self.idx_vocabulary is None:\n",
    "            self.__find_idx_vocabulary(sentences)\n",
    "            \n",
    "        vect = np.zeros((len(sentences), len(self.idx_vocabulary)),\n",
    "                       dtype=np.int32)\n",
    "\n",
    "        for i, s in tqdm(enumerate(sentences), desc='transform', position=0, \n",
    "                         leave=True, total=len(sentences)):\n",
    "            neg = 1\n",
    "            enh = 1\n",
    "            \n",
    "            for w in s.split():\n",
    "                if not w.isalpha():\n",
    "                    continue\n",
    "                idx = self.idx_vocabulary[w]\n",
    "                vect[i][idx] += 1\n",
    "#                 if self.is_negation and w in neg_words:\n",
    "#                     neg *= -1\n",
    "#                 elif self.is_enhance and w in enh_words:\n",
    "#                     enh *= self.enh_mult\n",
    "#                 elif w in self.vocabulary:\n",
    "#                     idx = np.where(self.vocabulary == w)\n",
    "#                     vect[i][idx] += neg * enh\n",
    "        return vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(model, train_x, train_y, test_x, test_y):\n",
    "    print(f'Train error: {np.count_nonzero(model.predict(train_x) != train_y) / len(train_y)}')\n",
    "    print(f'Test error:  {np.count_nonzero(model.predict(test_x) != test_y) / len(test_y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "transform: 100%|██████████| 7000/7000 [00:05<00:00, 1298.95it/s]\n",
      "transform: 100%|██████████| 1529/1529 [00:01<00:00, 1305.12it/s]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = np.array(list(word_count.keys()))\n",
    "\n",
    "cv = CustomCountVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "train_x = cv.transform(train_df['Phrase'])\n",
    "train_y = train_df['Sentiment'] * 4\n",
    "\n",
    "test_x = cv.transform(test_df['Phrase'])\n",
    "test_y = test_df['Sentiment'] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.8 s, sys: 1.29 s, total: 26.1 s\n",
      "Wall time: 8.94 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression(multi_class='multinomial', max_iter=1000, solver='lbfgs')\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.25971428571428573\n",
      "Test error:  0.631131458469588\n"
     ]
    }
   ],
   "source": [
    "compute_error(clf, train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCountVectorizer:\n",
    "    def __init__(self, min_df=-1, max_df=1e18, negative=False, enhance=False):\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        self.negative = negative\n",
    "        self.enhence_coeff = 5\n",
    "        self.enhance = enhance\n",
    "        \n",
    "    def fit(self, df):\n",
    "        words_cnt = defaultdict(int)\n",
    "        col = df.columns[0]\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            text = df.iloc[i][col]\n",
    "            for word in text.split():\n",
    "                words_cnt[word] += 1\n",
    "                \n",
    "        all_words = []\n",
    "        for word, cnt in words_cnt.items():\n",
    "            if self.min_df <= cnt <= self.max_df:\n",
    "                all_words.append(word)\n",
    "                \n",
    "        self.all_words_ids = {w:i for i,w in enumerate(all_words)}\n",
    "        self.width = len(all_words)\n",
    "        \n",
    "    \n",
    "    def transform(self, df):\n",
    "        col = df.columns[0]\n",
    "        count_matrix = np.zeros([len(df), self.width], \\\n",
    "                                dtype=np.int32)\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            text = df.iloc[i][col].split()\n",
    "\n",
    "            cnt_neg = 0\n",
    "            for word in text:\n",
    "                if word in self.all_words_ids:\n",
    "                    pos = self.all_words_ids[word]\n",
    "                    count_matrix[i][pos] += 1\n",
    "                    \n",
    "        return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvD = MyCountVectorizer()\n",
    "cvD.fit(train_df)\n",
    "trD = cvD.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6038571428571429"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrD = CustomLogisticRegression()\n",
    "lrD.fit(trD, train_y/4)\n",
    "(np.round(lrD.predict(trD)[1]*4) == train_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13831, 13695)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cvD.all_words_ids), len(cvG.idx_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "transform: 100%|██████████| 7000/7000 [00:00<00:00, 12448.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46085714285714285"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvG = CustomCountVectorizer()\n",
    "trG = cvG.transform(train_df['Phrase'])\n",
    "\n",
    "lrG = CustomLogisticRegression()\n",
    "lrG.fit(trG, train_y/4)\n",
    "(np.round(lrG.predict(trG)[1]*4) == train_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "','",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-493a3a0c0ff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcvD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_words_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: ','"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6038571428571429"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv = MyCountVectorizer()\n",
    "# cv.fit(train_df)\n",
    "\n",
    "# X_train = cv.transform(train_df) \n",
    "# X_test = cv.transform(test_df)\n",
    "\n",
    "cv = CustomCountVectorizer()\n",
    "X_train = cv.transform(train_df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 1.  , 0.5 , ..., 0.75, 0.75, 0.75])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(tmp_pr*4)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative words: ['incoherent' 'dull' 'stupid' 'devoid' 'worst']\n",
      "Most positive words: ['entertaining' 'best' 'beautifully' 'remarkable' 'masterpiece']\n"
     ]
    }
   ],
   "source": [
    "most_negs = vocabulary[clf.coef_[0].argsort()[-5:]]\n",
    "most_poss = vocabulary[clf.coef_[-1].argsort()[-5:]]\n",
    "print(f'Most negative words: {most_negs}')\n",
    "print(f'Most positive words: {most_poss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **[3p]** Now prepare an encoding in which negation flips the sign of the following words. For instance for our vocabulary the encodings become:\n",
    "\t```\n",
    "\tgood:                           [0,1,0,0,0,0,0]\n",
    "\tnot good:                       [0,-1,0,0,1,0,0]\n",
    "\tnot not good:                   [0,1,0,0,0,0,0]\n",
    "\tthe movie is not a funny movie: [1,0,0,1,1,-1,-1]\n",
    "\t```\n",
    "\tFor best results, you will probably need to construct a list of negative words.\n",
    "\t\n",
    "\tAgain train a logistic regression classifier and compare the results to the Bag of Words approach.\n",
    "\t\n",
    "\tPlease note that this model still maintains a single parameter for each word, but now the sentence score is\n",
    "\t$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}-1^{\\text{count of negations preceeding }w}S_w$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = ['no', 'none', 'not', 'never', 'nobody']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "transform: 100%|██████████| 7000/7000 [00:05<00:00, 1227.55it/s]\n",
      "transform: 100%|██████████| 1529/1529 [00:01<00:00, 1185.62it/s]\n"
     ]
    }
   ],
   "source": [
    "cv2 = CustomCountVectorizer(vocabulary=vocabulary, is_negation=True, neg_words=neg_words)\n",
    "train_x2 = cv2.transform(train_df['Phrase'])\n",
    "test_x2 = cv2.transform(test_df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 s, sys: 1.29 s, total: 25 s\n",
      "Wall time: 8.58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf2 = LogisticRegression(multi_class='multinomial', max_iter=1000, solver='lbfgs')\n",
    "clf2.fit(train_x2, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.2805714285714286\n",
      "Test error:  0.6402877697841727\n"
     ]
    }
   ],
   "source": [
    "compute_error(clf2, train_x2, train_y, test_x2, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **[5p]** Now also consider emphasizing words such as `very`. They can boost (multiply by a constant >1) the following words.\n",
    "\tImplement learning the modifying multiplier for negation and for emphasis. One way to do this is to introduce a model which has:\n",
    "\t- two modifiers, $N$ for negation and $E$ for emphasis\n",
    "\t- a sentiment score $S_w$ for each word \n",
    "And score each sentence as:\n",
    "$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}N^{\\text{#negs prec. }w}E^{\\text{#emphs prec. }w}S_w$$\n",
    "\n",
    "You will need to implement a custom logistic regression model to support it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "    \n",
    "    def __init__(self, Theta=None, max_iter=1000, solver=sopt.fmin_l_bfgs_b):\n",
    "        self.Theta = Theta\n",
    "        self.max_iter = max_iter\n",
    "        self.solver = solver\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def logreg_loss(self, Theta, X, Y):\n",
    "        Theta = Theta.astype(np.float64)\n",
    "        X = X.astype(np.float64)\n",
    "        Y = Y.astype(np.float64)\n",
    "        \n",
    "        Z = np.dot(X, Theta.T)\n",
    "\n",
    "        sig_Z = self.sigmoid(Z)\n",
    "        Y_ = Y[:,np.newaxis]\n",
    "        nll = -np.sum((Y_ * np.log2(sig_Z + 1e-50) + (1-Y_) * np.log2(1 - sig_Z + 1e-50)))\n",
    "        nll += np.sum(Theta**2) / 2\n",
    "        \n",
    "        grad = np.dot(X.T, (sig_Z-Y).T)\n",
    "        grad = grad.reshape(Theta.shape) + Theta\n",
    "\n",
    "        return nll / len(Y), grad / len(Y)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        Theta = self.Theta\n",
    "        if Theta is None:\n",
    "            Theta = np.ones(X.shape[1] + 1)\n",
    "            \n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        Theta = self.solver(lambda th: self.logreg_loss(th, X, Y), \n",
    "                            Theta,\n",
    "                            maxiter=self.max_iter)[0]\n",
    "        self.Theta = Theta\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        preds = np.dot(self.Theta, X.T)\n",
    "        return preds, self.sigmoid(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "transform: 100%|██████████| 7000/7000 [00:05<00:00, 1336.82it/s]\n",
      "transform: 100%|██████████| 1529/1529 [00:01<00:00, 1343.99it/s]\n"
     ]
    }
   ],
   "source": [
    "train_y = np.array(train_y) / np.max(train_y)\n",
    "\n",
    "enh_mult = 5\n",
    "cv3 = CustomCountVectorizer(vocabulary=voc, is_negation=True, is_enhance=True, \n",
    "                           neg_words=neg_words, enh_words=enh_words, enh_mult=enh_mult)\n",
    "train_x3 = cv3.transform(train_df['Phrase'])\n",
    "test_x3 = cv3.transform(test_df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakubgrodzicki/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.5 s, sys: 18 s, total: 51.4 s\n",
      "Wall time: 46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf3 = CustomLogisticRegression()\n",
    "clf3.fit(train_x3, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_preds = clf3.predict(train_x3)\n",
    "te_preds = clf3.predict(test_x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train err: 0.6297142857142857\n",
      "Test err: 0.9692609548724657\n"
     ]
    }
   ],
   "source": [
    "tr_err = (np.round(tr_preds[1]*4)/4 != train_y).mean()\n",
    "te_err = (np.round(te_preds[1]*4)/4 != test_y).mean()\n",
    "print(f'Train err: {tr_err}\\nTest err: {te_err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **[2pb]** Propose, implement, and evaluate an extension to the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWGhpTFbZqtH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Assignment1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
