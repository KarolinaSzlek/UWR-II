{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAiECkYEaZn1"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "## Important notes\n",
    "**Submission deadline:**\n",
    "* **Thursday, 12.03.2020**\n",
    "\n",
    "**Points: 13 + 2bp**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bb0zN1GvapSt"
   },
   "source": [
    "This assignment is meant to test your skills in course pre-requisites:  Scientific Python programming and  Machine Learning. If it is hard, I strongly advise you to drop the course.\n",
    "\n",
    "Please use GitHub’s [pull requests](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests) and issues to send corrections!\n",
    "\n",
    "You can solve the assignment in any system you like, but we encourage you to try out [Google Colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import re\n",
    "import scipy.optimize as sopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCountVectorizer():\n",
    "    \n",
    "    def __init__(self, is_negation=False, is_enhance=False, \n",
    "                 neg_words=[], enh_words=[], enh_mult=2, no_words=None):\n",
    "        self.is_negation = is_negation\n",
    "        self.is_enhance = is_enhance\n",
    "        self.neg_words = neg_words\n",
    "        self.enh_words = enh_words\n",
    "        self.enh_mult = enh_mult\n",
    "        self.no_words = no_words\n",
    "    \n",
    "    \n",
    "    def fit(self, sentences):\n",
    "        count = defaultdict(int)\n",
    "        for s in sentences:\n",
    "            for w in s.split():\n",
    "                count[w] += 1\n",
    "                \n",
    "        srt = sorted(count.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        if self.no_words is not None:\n",
    "            srt = srt[:self.no_words]\n",
    "        \n",
    "        self.idx_vocabulary = {w:i for i, (w, c) in enumerate(srt)}\n",
    "    \n",
    "    def transform(self, sentences):\n",
    "        vect = np.zeros((len(sentences), len(self.idx_vocabulary)))\n",
    "\n",
    "        for i, s in enumerate(sentences):\n",
    "            neg = 1\n",
    "            enh = 1\n",
    "            \n",
    "            for w in s.split():\n",
    "                if w not in self.idx_vocabulary:\n",
    "                    continue\n",
    "                \n",
    "                idx = self.idx_vocabulary[w]\n",
    "                \n",
    "                if self.is_negation and w in neg_words:\n",
    "                    neg *= -1\n",
    "                elif self.is_enhance and w in enh_words:\n",
    "                    enh *= self.enh_mult\n",
    "                else:\n",
    "                    vect[i][idx] += neg * enh\n",
    "        return vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "    \n",
    "    def __init__(self, Theta=None, max_iter=1000, solver=sopt.fmin_l_bfgs_b):\n",
    "        self.Theta = Theta\n",
    "        self.max_iter = max_iter\n",
    "        self.solver = solver\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def logreg_loss(self, Theta, X, Y):\n",
    "        Theta = Theta.astype(np.float64)\n",
    "        X = X.astype(np.float64)\n",
    "        Y = Y.astype(np.float64)\n",
    "        \n",
    "        Z = np.dot(X, Theta.T)\n",
    "\n",
    "        sig_Z = self.sigmoid(Z)\n",
    "        Y_ = Y[:,np.newaxis]\n",
    "        nll = -np.sum((Y_ * np.log2(sig_Z + 1e-50) + (1-Y_) * np.log2(1 - sig_Z + 1e-50)))\n",
    "        nll += np.sum(Theta**2) / 2\n",
    "        \n",
    "        grad = np.dot(X.T, (sig_Z-Y).T)\n",
    "        grad = grad.reshape(Theta.shape) + Theta\n",
    "\n",
    "        return nll / len(Y), grad / len(Y)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        Theta = self.Theta\n",
    "        if Theta is None:\n",
    "            Theta = np.ones(X.shape[1] + 1)\n",
    "            \n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        Theta = self.solver(lambda th: self.logreg_loss(th, X, Y), \n",
    "                            Theta,\n",
    "                            maxiter=self.max_iter)[0]\n",
    "        self.Theta = Theta\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        preds = np.dot(self.Theta, X.T)\n",
    "        return preds, self.sigmoid(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIJIJDwgZrOI"
   },
   "source": [
    "1. **[1p]** Download data competition from a Kaggle competition on sentiment prediction from [[https://www.kaggle.com/c/CountVectorizerent-analysis-on-movie-reviews/data](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)].  Keep only full sentences, i.e. for each `SenteceId` keep only the entry with the lowest `PhraseId`.  Use first 7000 sentences as a `train set` and the remaining 1529 sentences as the `test set`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(s):\n",
    "    return re.sub('\\ *\\W\\ *', ' ', s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "df = df.groupby(['SentenceId'], as_index=False).agg({'PhraseId' : 'min',\n",
    "                                               'Phrase' : 'first',\n",
    "                                               'Sentiment' : 'first'})\n",
    "df['Sentiment'] /= 4\n",
    "df = df.drop(['PhraseId', 'SentenceId'], axis=1)\n",
    "df['Phrase'] = df['Phrase'].apply(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:7000]\n",
    "test_df = df.iloc[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this quiet introspective and entertaining inde...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>even fans of ismail merchant s work i suspect ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a positively thrilling combination of ethnogra...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aggressive self glorification and a manipulati...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment\n",
       "0  a series of escapades demonstrating the adage ...       0.25\n",
       "1  this quiet introspective and entertaining inde...       1.00\n",
       "2  even fans of ismail merchant s work i suspect ...       0.25\n",
       "3  a positively thrilling combination of ethnogra...       0.75\n",
       "4  aggressive self glorification and a manipulati...       0.25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **[1p]** Prepare the data for logistic regression:\n",
    "\tMap the sentiment scores $0,1,2,3,4$ to a probability of the sentence being by setting $p(\\textrm{positive}) = \\textrm{sentiment}/4$.\n",
    "\tBuild a dictionary of most frequent 20000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [00:01<00:00, 6230.65it/s]\n"
     ]
    }
   ],
   "source": [
    "word_count = defaultdict(int)\n",
    "for i in trange(len(train_df), position=0, leave=True):\n",
    "    for w in train_df.iloc[i]['Phrase'].split():\n",
    "        word_count[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = dict(sorted(word_count.items(), key=lambda kv: kv[1], reverse=True)[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **[3p]** Treat each document as a bag of words. e.g. if the vocabulary is \n",
    "\t```\n",
    "\t0: the\n",
    "\t1: good\n",
    "\t2: movie\n",
    "\t3: is\n",
    "\t4: not\n",
    "\t5: a\n",
    "\t6: funny\n",
    "\t```\n",
    "\tThen the encodings can be:\n",
    "\t```\n",
    "\tgood:                           [0,1,0,0,0,0,0]\n",
    "\tnot good:                       [0,1,0,0,1,0,0] \n",
    "\tthe movie is not a funny movie: [1,0,2,1,1,1,1]\n",
    "\t```\n",
    "    Train a logistic regression model to predict the sentiment. Compute the correlation between the predicted probabilities and the sentiment. Record the most positive and negative words.\n",
    "    Please note that in this model each word gets its sentiment parameter $S_w$ and the score for a sentence is \n",
    "    $$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}S_w$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df['Sentiment']\n",
    "test_y = test_df['Sentiment']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "train_x = cv.fit_transform(train_df['Phrase'])\n",
    "test_x = cv.transform(test_df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.28 s, sys: 579 ms, total: 6.86 s\n",
      "Wall time: 2.33 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression(multi_class='multinomial', max_iter=1000, solver='lbfgs')\n",
    "clf.fit(train_x, train_y*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Logistic Regression\n",
      "Confusion matrix:\n",
      "[[ 834   28   12    3    1]\n",
      " [   6 1771   19   14    0]\n",
      " [   3   38 1296   23    2]\n",
      " [   2   19   14 1854   13]\n",
      " [   1    8   13   28  998]]\n",
      "Accuracy for train: 0.9647142857142857\n",
      "Accuracy for test: 0.38848920863309355\n"
     ]
    }
   ],
   "source": [
    "preds_tr = clf.predict(train_x)\n",
    "print('Sklearn Logistic Regression')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(train_y*4, preds_tr)}')\n",
    "print(f'Accuracy for train: {(train_y*4 == preds_tr).mean()}')\n",
    "print(f'Accuracy for test: {(test_y*4 == clf.predict(test_x)).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_cv = CustomCountVectorizer()\n",
    "c_cv.fit(train_df['Phrase'])\n",
    "c_train_x = c_cv.transform(train_df['Phrase'])\n",
    "c_test_x = c_cv.transform(test_df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 54s, sys: 2min 1s, total: 5min 55s\n",
      "Wall time: 4min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "c_clf = CustomLogisticRegression()\n",
    "c_clf.fit(c_train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Logistic Regression\n",
      "Confusion matrix:\n",
      "[[ 281  554   43    0    0]\n",
      " [  33 1213  556    8    0]\n",
      " [   1  183 1040  138    0]\n",
      " [   1    7  552 1308   34]\n",
      " [   0    0   52  611  385]]\n",
      "Accuracy for train: 0.6038571428571429\n",
      "Accuracy for test: 0.3597122302158273\n"
     ]
    }
   ],
   "source": [
    "c_preds_tr = np.round(c_clf.predict(c_train_x)[1]*4)/4\n",
    "c_preds_te = np.round(c_clf.predict(c_test_x)[1]*4)/4\n",
    "print('Custom Logistic Regression')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(train_y*4, c_preds_tr*4)}')\n",
    "print(f'Accuracy for train: {(train_y == c_preds_tr).mean()}')\n",
    "print(f'Accuracy for test: {(test_y == c_preds_te).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **[3p]** Now prepare an encoding in which negation flips the sign of the following words. For instance for our vocabulary the encodings become:\n",
    "\t```\n",
    "\tgood:                           [0,1,0,0,0,0,0]\n",
    "\tnot good:                       [0,-1,0,0,1,0,0]\n",
    "\tnot not good:                   [0,1,0,0,0,0,0]\n",
    "\tthe movie is not a funny movie: [1,0,0,1,1,-1,-1]\n",
    "\t```\n",
    "\tFor best results, you will probably need to construct a list of negative words.\n",
    "\t\n",
    "\tAgain train a logistic regression classifier and compare the results to the Bag of Words approach.\n",
    "\t\n",
    "\tPlease note that this model still maintains a single parameter for each word, but now the sentence score is\n",
    "\t$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}-1^{\\text{count of negations preceeding }w}S_w$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = ['no', 'none', 'not', 'never', 'nobody']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CustomCountVectorizer(is_negation=True, neg_words=neg_words)\n",
    "cv2.fit(train_df['Phrase'])\n",
    "train_x2 = cv2.transform(train_df['Phrase'])\n",
    "test_x2 = cv2.transform(test_df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 57s, sys: 3.64 s, total: 2min\n",
      "Wall time: 50.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf2 = LogisticRegression(multi_class='multinomial', max_iter=1000, solver='lbfgs')\n",
    "clf2.fit(train_x2, train_y*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Logistic Regression\n",
      "Confusion matrix:\n",
      "[[ 830   29   10    8    1]\n",
      " [   3 1778   17   12    0]\n",
      " [   5   44 1288   21    4]\n",
      " [   5   17   18 1849   13]\n",
      " [   2   13   16   26  991]]\n",
      "Accuracy for train: 0.9622857142857143\n",
      "Accuracy for test: 0.36625245258338784\n"
     ]
    }
   ],
   "source": [
    "preds_tr2 = clf2.predict(train_x2)\n",
    "print('Sklearn Logistic Regression')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(train_y*4, preds_tr2)}')\n",
    "print(f'Accuracy for train: {(train_y*4 == preds_tr2).mean()}')\n",
    "print(f'Accuracy for test: {(test_y*4 == clf2.predict(test_x2)).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 45.2 s, total: 2min 11s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "c_clf2 = CustomLogisticRegression()\n",
    "c_clf2.fit(train_x2, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Logistic Regression\n",
      "Confusion matrix:\n",
      "[[ 333  456   84    4    1]\n",
      " [ 146 1020  587   56    1]\n",
      " [   6  283  816  250    7]\n",
      " [   1   39  532 1201  129]\n",
      " [   0    1   70  515  462]]\n",
      "Accuracy for train: 0.5474285714285714\n",
      "Accuracy for test: 0.35317200784826686\n"
     ]
    }
   ],
   "source": [
    "c_preds_tr2 = np.round(c_clf2.predict(train_x2)[1]*4)/4\n",
    "c_preds_te2 = np.round(c_clf2.predict(test_x2)[1]*4)/4\n",
    "print('Custom Logistic Regression')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(train_y*4, c_preds_tr2*4)}')\n",
    "print(f'Accuracy for train: {(train_y == c_preds_tr2).mean()}')\n",
    "print(f'Accuracy for test: {(test_y == c_preds_te2).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **[5p]** Now also consider emphasizing words such as `very`. They can boost (multiply by a constant >1) the following words.\n",
    "\tImplement learning the modifying multiplier for negation and for emphasis. One way to do this is to introduce a model which has:\n",
    "\t- two modifiers, $N$ for negation and $E$ for emphasis\n",
    "\t- a sentiment score $S_w$ for each word \n",
    "And score each sentence as:\n",
    "$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}N^{\\text{#negs prec. }w}E^{\\text{#emphs prec. }w}S_w$$\n",
    "\n",
    "You will need to implement a custom logistic regression model to support it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "enh_words = ['extremely', 'exceedingly', 'exceptionally', 'especially', 'tremendously', 'immensely', 'vastly', 'hugely', 'extraordinarily', 'extra', 'excessively', 'overly', 'over', 'abundantly', 'inordinately', 'singularly', 'significantly', 'distinctly', 'outstandingly', 'uncommonly', 'unusually', 'decidedly', 'particularly', 'eminently', 'supremely', 'highly', 'remarkably', 'really', 'truly', 'mightily', 'thoroughly', 'all that', 'to a great extent', 'most', 'so', 'too', 'unco', 'très', 'right', 'terrifically', 'awfully', 'terribly', 'devilishly', 'madly', 'majorly', 'seriously', 'desperately', 'mega', 'ultra', 'oh-so', 'too-too', 'stinking', 'mucho', 'damn', 'damned', 'too … for words', 'devilish', 'hellish', 'frightfully', 'ever so', 'well', 'bloody', 'dead', 'dirty', 'jolly', 'fair', 'real', 'mighty', 'powerful', 'awful', 'plumb', 'darned', 'way', 'bitching', 'mad', 'lekker', 'exceeding', 'sore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv3 = CustomCountVectorizer(is_negation=True, neg_words=neg_words, is_enhance=True,\n",
    "                           enh_words=enh_words, enh_mult=2)\n",
    "cv3.fit(train_df['Phrase'])\n",
    "train_x3 = cv3.transform(train_df['Phrase'])\n",
    "test_x3 = cv3.transform(test_df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 30s, sys: 1min 19s, total: 3min 50s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf3 = CustomLogisticRegression()\n",
    "clf3.fit(train_x3, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Logistic Regression\n",
      "Confusion matrix:\n",
      "[[ 274  544   59    1    0]\n",
      " [  13 1152  632   13    0]\n",
      " [   0  128 1123  111    0]\n",
      " [   0    8  591 1277   26]\n",
      " [   0    0   51  633  364]]\n",
      "Accuracy for train: 0.5985714285714285\n",
      "Accuracy for test: 0.3276651406147809\n"
     ]
    }
   ],
   "source": [
    "preds_tr3 = np.round(clf3.predict(train_x3)[1]*4)/4\n",
    "preds_te3 = np.round(clf3.predict(test_x3)[1]*4)/4\n",
    "print('Custom Logistic Regression')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(train_y*4, preds_tr3*4)}')\n",
    "print(f'Accuracy for train: {(train_y == preds_tr3).mean()}')\n",
    "print(f'Accuracy for test: {(test_y == preds_te3).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **[2pb]** Propose, implement, and evaluate an extension to the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWGhpTFbZqtH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Assignment1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
